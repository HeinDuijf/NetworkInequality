{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e202e3dc",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234ae27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95dc7ff",
   "metadata": {},
   "source": [
    "# Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f41c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "def plot_network_degree_distribution(G, directed=True, title='title'):\n",
    "    if directed:\n",
    "        degrees = np.array([degree for node, degree in G.out_degree()])\n",
    "    else:\n",
    "        degrees = np.array([degree for node, degree in G.degree()])\n",
    "    # Create the histogram with a KDE\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(degrees, kde=False, bins=150, stat=\"count\")\n",
    "    # Calculate the mean\n",
    "    mean_value = np.mean(degrees)\n",
    "    print(mean_value)\n",
    "    print(np.median(degrees))\n",
    "\n",
    "    # Plot a vertical line at the mean value\n",
    "    plt.axvline(mean_value, color='b', linestyle='--', linewidth=2)\n",
    "    plt.text(mean_value + 0.1, plt.ylim()[1] * 0.9, f'Mean: {mean_value}', color='b')\n",
    "    # plt.text(mean_value + 0.1, plt.ylim()[1] * 0.9, 'Mean: {:.2f}'.format(mean_value), color='b')\n",
    "\n",
    "    plt.title('Timeline Smooth Histogram for: ' + title)\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(fontsize=8,rotation=20)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loglog(G,directed=True,m=10):\n",
    "    if directed:\n",
    "        # Get the in-degree of all nodes\n",
    "        out_degrees = [d for _, d in G.out_degree()]\n",
    "\n",
    "        # Compute the histogram\n",
    "        max_degree = max(out_degrees)\n",
    "        degree_freq = [out_degrees.count(i) for i in range(max_degree + 1)]\n",
    "    else:\n",
    "        degree_freq = nx.degree_histogram(G)\n",
    "    degrees = range(len(degree_freq))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.loglog(degrees[m:], degree_freq[m:],'go-')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Log-Log plot of the degree distribution')\n",
    "    \n",
    "    \n",
    "    def scatter_plot(df, target_variable=\"share_of_correct_agents_at_convergence\"):\n",
    "        # Select numerical columns excluding unique ID and target variable\n",
    "        numerical_columns = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        numerical_columns.remove(target_variable)  # Remove target variable from independent variables\n",
    "\n",
    "        # Generate scatter plots for each numerical column against the target variable\n",
    "        num_plots = len(numerical_columns)\n",
    "        fig, axes = plt.subplots(nrows=(num_plots + 1) // 2, ncols=2, figsize=(10, num_plots * 2))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, column in enumerate(numerical_columns):\n",
    "            axes[i].scatter(df[column], df[target_variable], alpha=0.5)\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel(target_variable)\n",
    "            axes[i].set_title(f\"{column} vs {target_variable}\")\n",
    "            axes[i].grid(True)\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313b67b",
   "metadata": {},
   "source": [
    "# Network Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b950d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network statistics\n",
    "def calculate_degree_gini(G, directed = True):\n",
    "    if directed:\n",
    "        degrees = [deg for _, deg in G.out_degree()]\n",
    "    else:\n",
    "        degrees = [deg for _, deg in G.degree()]\n",
    "    # Sort the degrees in ascending order\n",
    "    sorted_x = np.sort(np.array(degrees))\n",
    "    n = len(np.array(degrees))\n",
    "    cumx = np.cumsum(sorted_x, dtype=float)\n",
    "    gini = (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n",
    "\n",
    "    return gini\n",
    "\n",
    "def find_reachability_dominator_set(G):\n",
    "    \"\"\"\n",
    "    Finds a minimal reachability dominator set in a directed graph G.\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.DiGraph): A directed graph.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of nodes A such that every node in G is reachable from some node in A.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute strongly connected components\n",
    "    sccs = list(nx.strongly_connected_components(G))\n",
    "\n",
    "    # Step 2: Build the condensation graph\n",
    "    C = nx.condensation(G, sccs)\n",
    "\n",
    "    # Step 3: Find source SCCs (no incoming edges)\n",
    "    source_sccs = [node for node in C.nodes if C.in_degree(node) == 0]\n",
    "\n",
    "    # Step 4: Pick one representative node from each source SCC\n",
    "    reachability_dominator_set = set()\n",
    "    scc_list = C.graph['mapping']  # maps node -> scc index\n",
    "    inverse_scc_map = {}\n",
    "    for node, scc_id in scc_list.items():\n",
    "        inverse_scc_map.setdefault(scc_id, []).append(node)\n",
    "\n",
    "    for source_scc in source_sccs:\n",
    "        representative = inverse_scc_map[source_scc][0]  # pick one node from this SCC\n",
    "        reachability_dominator_set.add(representative)\n",
    "\n",
    "    return len(reachability_dominator_set), len(reachability_dominator_set)/len(G), len(C), len(C)/len(G)\n",
    "\n",
    "def network_statistics(G, directed = True):\n",
    "    stats = {}\n",
    "\n",
    "    # Average degree\n",
    "    if directed:\n",
    "        degrees = [deg for _, deg in G.out_degree()]\n",
    "    else:\n",
    "        degrees = [deg for _, deg in G.degree()]\n",
    "    stats['average_degree'] = sum(degrees) / len(degrees)\n",
    "\n",
    "    # Gini coefficient\n",
    "    #print(degrees)\n",
    "    stats['degree_gini_coefficient'] = calculate_degree_gini(G, directed=directed)\n",
    "\n",
    "    # Compute clustering for each node\n",
    "    # it allows us to use weights, which we neglect...\n",
    "    clustering_values = nx.clustering(G)\n",
    "    # Compute the average clustering coefficient manually\n",
    "    average_clustering = sum(clustering_values.values()) / len(clustering_values)\n",
    "    stats['approx_average_clustering_coefficient'] = average_clustering\n",
    "\n",
    "    if directed:    \n",
    "        if nx.is_strongly_connected(G):\n",
    "            stats['avg_path_length'] = nx.average_shortest_path_length(G)\n",
    "        else:\n",
    "            stats['avg_path_length'] = len(G.nodes)+1\n",
    "            # largest_component = max(nx.weakly_connected_components(G), key=len)\n",
    "            # subgraph = G.subgraph(largest_component)\n",
    "            # stats['diameter'] = nx.diameter(subgraph)\n",
    "    else:\n",
    "        if nx.is_connected(G):\n",
    "            stats['avg_path_length'] = nx.average_shortest_path_length(G)\n",
    "        else:\n",
    "            stats['avg_path_length'] = len(G.nodes)+1\n",
    "            # largest_component = max(nx.connected_components(G), key=len)\n",
    "            # subgraph = G.subgraph(largest_component)\n",
    "            # stats['diameter'] = nx.diameter(subgraph)\n",
    "\n",
    "    if directed:\n",
    "        out_degrees = np.array([d for _, d in G.out_degree()])\n",
    "        # out_degrees = np.array([d for _, d in graph.out_degree()])\n",
    "        in_hist, _ = np.histogram(out_degrees, bins=range(np.max(out_degrees) + 2), density=True)\n",
    "        # out_hist, _ = np.histogram(out_degrees, bins=range(np.max(out_degrees) + 2), density=True)\n",
    "        out_entropy = -np.sum(in_hist[in_hist > 0] * np.log(in_hist[in_hist > 0]))\n",
    "        # out_entropy = -np.sum(out_hist[out_hist > 0] * np.log(out_hist[out_hist > 0]))\n",
    "        stats['degree_entropy'] = out_entropy\n",
    "    else:\n",
    "        degrees = np.array([d for _, d in G.degree()])\n",
    "        hist, _ = np.histogram(degrees, bins=range(np.max(degrees) + 2), density=True)\n",
    "        entropy = -np.sum(hist[hist > 0] * np.log(hist[hist > 0]))\n",
    "        stats['degree_entropy'] = entropy\n",
    "\n",
    "    # Add additional metrics as needed here, e.g., centrality measures\n",
    "    stats['reachability_dominator_set_size'] = find_reachability_dominator_set(G)[0]\n",
    "    stats['reachability_dominator_set_ratio'] = find_reachability_dominator_set(G)[1]\n",
    "    stats['condensation_graph_size'] = find_reachability_dominator_set(G)[2]\n",
    "    stats['condensation_graph_ratio'] = find_reachability_dominator_set(G)[3]\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304f3dc",
   "metadata": {},
   "source": [
    "# Variation Methods\n",
    "\n",
    "Main contribution of the paper:\n",
    "\n",
    "- Methodological contribution of empirical robustness for _network_ epistemology. \n",
    "\n",
    "Key challenge:\n",
    "\n",
    "- Test whether inequality (or some other network feature such as density, clustering, or diameter) increases reliability (or some other metric such as speed).\n",
    "- Counterfactual: had the network been more equal, the group would have been more reliable.\n",
    "- This requires us to identify networks that differ in their inequality but are otherwise maximally similar.\n",
    "- We consider three network features: density, clustering, and degree inequality. So networks that have the similar density, clustering and degree inequality are considered maximally similar. (I left out diameter here.)\n",
    "\n",
    "To achieve this, we develop some variation methods. The main goals for these variation methods are:\n",
    "\n",
    "1. Control: Ability to tinker with specific network features (density, inequality, clustering)\n",
    "2. Simplicity\n",
    "3. Computational tractability\n",
    "4. Link to (individualistic) intervention \n",
    "\n",
    "There are two types of variation methods: \n",
    "\n",
    "1. Possibly complex variation methods that can be used to produce networks with specific network properties (density, inequality, clustering), i.e., high control\n",
    "2. Intuitive and simple variation methods that yield a lower degree of control over the specific network properties (density, inequality, clustering)\n",
    "\n",
    "The basic ideas in these variation methods are the following:\n",
    "\n",
    "1. Density\n",
    "    - Increase by adding edges\n",
    "    - Keep constant by rewiring edges\n",
    "    - Decrease by removing edges [not implemented]\n",
    "2. Clustering\n",
    "    - Note: the local clustering coefficient of a given node basically is the number of triangles that pass by that node divided by the number of possible triangles that pass by that node. \n",
    "    - Increase by adding edges that create new triangles\n",
    "    - Keep constant ???\n",
    "    - Decrease by removing edges from existing triangles\n",
    "3. Inequality\n",
    "    - Decrease by adding edges randomly (following the uniform degree distribution)\n",
    "    - Keep constant by adding edges following the original degree distribution (i.e., that of the PUD network)\n",
    "    - Increase by (a) sequentially adding edges preferentially, or  (b) by adding edges following a degree distribution that is more unequal than the original one [neither implemented]\n",
    "\n",
    "## Hop over to Outcomes section to see the results of the different variation methods\n",
    "\n",
    "## Simple variation methods\n",
    "\n",
    "### Equalizers: tinker with inequality\n",
    "\n",
    "1. `randomize_network`: Randomly rewire edges (following the uniform degree distribution) [THIS IS WHAT WE CURRENTLY DO]\n",
    "    - Density $=$\n",
    "    - Clustering $\\downarrow$\n",
    "    - Inequality $\\downarrow$\n",
    "2. `equalize`: Rewire triangles: Take a triangle, take a node in the triangle, remove the edge in the triangle that does not contain the node, then add a random new edge that creates a new triangle that passed by the node. (The goal was to keep clustering somewhat equal, while increasing equality.)\n",
    "    - Density $=$\n",
    "    - Clustering $\\approx \\downarrow$\n",
    "    - Inequality $\\downarrow$\n",
    "\n",
    "### Densify\n",
    "1. [not implemented] Randomly add edges (following the uniform degree distribution)\n",
    "    - Expectations\n",
    "        - Density $\\uparrow$\n",
    "        - Clustering $\\downarrow$\n",
    "        - Inequality $\\downarrow$\n",
    "2. `densify`: Add edges following the original degree distribution\n",
    "    - Density $\\uparrow$\n",
    "    - Clustering $\\downarrow$\n",
    "    - Inequality $=$\n",
    "3. `cluster`: Add edges that create new triangles (taking into account the original degree distribution)\n",
    "    - Density $\\uparrow$\n",
    "    - Clustering $\\Uparrow$\n",
    "    - Inequality $\\approx$ \n",
    "\n",
    "### Clustering\n",
    "1. `decluster`: Remove edges from existing triangles and add new edge following the original degree distribution\n",
    "    - Density $=$\n",
    "    - Clustering $\\Downarrow$\n",
    "    - Inequality $\\approx\\downarrow$\n",
    "\n",
    "\n",
    "## Complex variation methods\n",
    "\n",
    "1. `densify_fancy`: Add edges in such a way to attempt to reach a target clustering coefficient and a target degree distribution (original or uniform). \n",
    "    - Basically, the algorithm checks whether the target clustering coefficient has been reached. If not, it adds an edge that increases clustering. If yes, it adds a new edge following the target degree distribution. \n",
    "    - **Computationally costly**: after an edge is added, the algorithm calculates the clustering coefficient of the new network, which is computationally costly to an unacceptable degree (I think). \n",
    "    - **High degree of control**, especially regarandoming the clustering coefficient: For example, can be used to achieve the following:\n",
    "        1. Only increase density\n",
    "            - Density $\\uparrow$\n",
    "            - Clustering $=$\n",
    "            - Inequality $=$\n",
    "        2. Increase density and decrease inequality\n",
    "            - Density $\\uparrow$\n",
    "            - Clustering $=$\n",
    "            - Inequality $\\downarrow$\n",
    "2. `densify_fancy_speed_up`: This method basically works the same as `densify_fancy`, but has a significant speed up. Instead of calculating the average clustering coefficient whenever an edge is added, we only calculate the new local clusterinf coefficient for nodes that are affected by the newly added edge. \n",
    "    - **Computationally not cheap**: although the algorithm is much quicker than `densify_fancy`, it still takes some time. I expect that the computational costs are acceptable.\n",
    "    - **High control**, especially regarandoming the clustering coefficient.\n",
    "3. `densify_semi_fancy`: Add edges either to increase clustering or following the target degree distribution — with a fixed probability.\n",
    "    - Basically, the algorithm throws a (biased) coin to determine whether it will add an edge that increases clustering or add one following the target degree distribution. \n",
    "    - Computationally fast. Relies on a biased coin toss instead of calculating the clustering coefficient. The drawback is that the clustering coefficient will change to some degree.\n",
    "    - Pretty high degree of control.\n",
    "\n",
    "\n",
    "- Note: we could keep the density fixed by first removing a number of edges and then using `densify_fancy` or `densify_semi_fancy` to add the same number of edges. \n",
    "\n",
    "## Thoughts and observations\n",
    "\n",
    "- I was surprised to learn that both the in-degree and the out-degree distributions of the PUD network are scale free. However, they are not neatly correlated. Nonetheless, my suspicion is that there is a pattern between an agent’s number of publication and both its in-degree and its out-degree. \n",
    "- I was surprised to learn that the clustering coefficient is not based on directed graphs (`nx.average_clustering`). \n",
    "- In a sense, we have currently only implemented the target degree distribution of the degree distribution of the input network (the PUD network) or the uniform distribution. We could consider other degree distributions (i.e., an unequal distribution that flips the original degree distribution)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07cc1e",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee7d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triangles(net: nx.DiGraph):\n",
    "    \"\"\"Return the list of all triangles in a directed graph G.\"\"\"\n",
    "    triangles = []\n",
    "    for clique in nx.enumerate_all_cliques(net.to_undirected()):\n",
    "        if len(clique) <= 3:\n",
    "            if len(clique) == 3:\n",
    "                triangles.append(clique)\n",
    "        else:\n",
    "            return triangles\n",
    "    return triangles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315803e",
   "metadata": {},
   "source": [
    "## Randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8bd9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_network(G, p_rewiring):\n",
    "    # Check if the graph is directed\n",
    "    is_directed = G.is_directed()\n",
    "\n",
    "    # Get edges and nodes\n",
    "    edges = list(G.edges()).copy()\n",
    "    random.shuffle(edges)\n",
    "    edges_set = set(edges)\n",
    "    new_edges_set = edges_set.copy()\n",
    "    nodes = list(G.nodes()).copy()\n",
    "\n",
    "    # Find which edges to remove\n",
    "    to_remove_set = set()\n",
    "    for old_edge in edges:\n",
    "        if random.random() < p_rewiring:  # p probability to rewire an edge\n",
    "            to_remove_set.add(old_edge)\n",
    "            new_edges_set.remove(old_edge)\n",
    "\n",
    "    # Generate a new edges\n",
    "    for edge in to_remove_set:\n",
    "        new_edge = (random.choice(nodes), random.choice(nodes))\n",
    "        if not is_directed:\n",
    "            new_edge = tuple(sorted(new_edge))  # Ensure (u, v) == (v, u) for undirected graphs\n",
    "\n",
    "        # Avoid duplicate edges and self-loops\n",
    "        while (new_edge in new_edges_set) or (new_edge[0] == new_edge[1]):\n",
    "            new_edge = (random.choice(nodes), random.choice(nodes))\n",
    "            if not is_directed:\n",
    "                new_edge = tuple(sorted(new_edge))\n",
    "\n",
    "        new_edges_set.add(new_edge)\n",
    "\n",
    "    # Create a new graph with updated edges\n",
    "    G_new = G.copy()\n",
    "    G_new.remove_edges_from(to_remove_set)\n",
    "    G_new.add_edges_from(new_edges_set)\n",
    "\n",
    "    return G_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b2ee2",
   "metadata": {},
   "source": [
    "## Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee79c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(net: nx.DiGraph, n: int) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Equalize the network by rewiring n random edges.\n",
    "    \"\"\"\n",
    "    equalized_net = copy.deepcopy(net)\n",
    "    triangles = get_triangles(net)\n",
    "    rewired_triangles = random.sample(triangles, n)\n",
    "    \n",
    "    for triangle in rewired_triangles:\n",
    "        edge = triangle[-2:]  # Take the last two nodes as the edge to be rewired\n",
    "        # Remove edge\n",
    "        if equalized_net.has_edge(*edge):\n",
    "            equalized_net.remove_edge(*edge)\n",
    "        elif equalized_net.has_edge(edge[1], edge[0]):\n",
    "            equalized_net.remove_edge(edge[1], edge[0]) \n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Add new edge to create a new triangle that passes by the first node\n",
    "        node = triangle[0] \n",
    "        neighbors = list(net.predecessors(node)) + list(net.successors(node))\n",
    "\n",
    "        sources_sample = random.choices(neighbors, k=10)\n",
    "        targets_sample = random.choices(neighbors, k=10)\n",
    "        edge_sample = [\n",
    "            (source, target) \n",
    "            for source in sources_sample \n",
    "            for target in targets_sample \n",
    "            if source != target and not equalized_net.has_edge(source, target)]\n",
    "        new_edge = random.choice(edge_sample) # Throws an error if no edges are available\n",
    "        equalized_net.add_edge(*new_edge)\n",
    "    return equalized_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f0885",
   "metadata": {},
   "source": [
    "## Densify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dea5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify_network(net: nx.DiGraph, n_edges: int) -> nx.DiGraph:\n",
    "    # Create a copy of the original network\n",
    "    densified_net = copy.deepcopy(net)\n",
    "    \n",
    "    # Get the degree distribution\n",
    "    in_degrees = dict(net.in_degree())\n",
    "    out_degrees = dict(net.out_degree())\n",
    "    multiplier = 10\n",
    "    targets = random.choices(\n",
    "        list(in_degrees.keys()), \n",
    "        weights=list(in_degrees.values()), \n",
    "        k=multiplier*n_edges\n",
    "    )\n",
    "    sources = random.choices(\n",
    "        list(out_degrees.keys()), \n",
    "        weights=out_degrees.values(), \n",
    "        k=multiplier*n_edges\n",
    "    )\n",
    "\n",
    "    edges_new = list(set(zip(sources, targets))) \n",
    "    edges_new = [edge for edge in edges_new if edge[0] != edge[1]]  \n",
    "    edges_new = [edge for edge in edges_new if not edge in net.edges()]\n",
    "    edges_new = edges_new[:n_edges] \n",
    "    \n",
    "    densified_net.add_edges_from(edges_new)\n",
    "    return densified_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7144d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify_semi_fancy(\n",
    "    net: nx.DiGraph, n_edges: int, p_increase_clustering: float, target_degree_dist: str = \"original\",\n",
    ") -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Densifies a directed network by adding new edges, balancing between increasing \n",
    "    ing\n",
    "    and preserving a target degree distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : nx.DiGraph\n",
    "        The original directed network to densify.\n",
    "    n_edges : int\n",
    "        The number of new edges to add.\n",
    "    p_increase_clustering : float\n",
    "        Probability (between 0 and 1) that a new edge is added to increase clustering\n",
    "        (i.e., create new triangles). Otherwise, new edges are added based on the \n",
    "        target degree distribution.\n",
    "    target_degree_dist : str, optional\n",
    "        The target degree distribution for new edges.\n",
    "        \"original\" uses the original network's degree distribution,\n",
    "        \"uniform\" assigns equal probability to all nodes. Default is \"original\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nx.DiGraph\n",
    "        A new directed network with additional edges.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original network\n",
    "    net_new = copy.deepcopy(net)\n",
    "    \n",
    "    if target_degree_dist == \"original\":\n",
    "        # Use the original degree distribution\n",
    "        out_degrees = dict(net.out_degree())\n",
    "        in_degrees = dict(net.in_degree())\n",
    "    if target_degree_dist == \"uniform\":\n",
    "        out_degrees = {node: 1 for node in net.nodes()}\n",
    "        in_degrees = {node: 1 for node in net.nodes()}\n",
    "\n",
    "    # Add edges in neighborhoods\n",
    "    n_edges_added = 0\n",
    "    edges_added_clustering = 0\n",
    "    edges_added_degree_dist = 0\n",
    "    while n_edges_added < n_edges:\n",
    "        if random.random() < p_increase_clustering:\n",
    "            # Add new edge to increase clustering\n",
    "            possible_edges = []\n",
    "            while possible_edges == []:\n",
    "                node = random.choice(list(net.nodes()))\n",
    "                neighbors = list(net.predecessors(node)) + list(net.successors(node))\n",
    "                out_degrees_neighbors = {node: out_degrees[node] for node in neighbors}\n",
    "                in_degrees_neighbors = {node: in_degrees[node] for node in neighbors}\n",
    "                out_weights = out_degrees_neighbors.values()\n",
    "                if all(out_weights) == 0:\n",
    "                    out_weights = np.ones(len(out_degrees_neighbors.keys()))\n",
    "                \n",
    "                in_weights = in_degrees_neighbors.values()\n",
    "                if all(in_weights) == 0:\n",
    "                    in_weights = np.ones(len(in_degrees_neighbors.keys()))\n",
    "                \n",
    "                sources = random.choices(list(out_degrees_neighbors.keys()), weights=out_weights, k=10)\n",
    "                targets = random.choices(list(in_degrees_neighbors.keys()), weights=in_weights, k=10)\n",
    "                possible_edges = [\n",
    "                    (source, target) for source in sources for target in targets\n",
    "                    if source != target and not net_new.in_edges(source, target)\n",
    "                ]\n",
    "                if possible_edges != []:\n",
    "                    new_edge = random.choice(possible_edges)\n",
    "                    n_edges_added += 1\n",
    "                    net_new.add_edge(*new_edge)\n",
    "                    edges_added_clustering += 1\n",
    "        else:\n",
    "            # Add new edge based on target degree distribution\n",
    "            edge_sample = []\n",
    "            while edge_sample == []:\n",
    "                sources_sample = random.choices(list(out_degrees.keys()), weights=out_degrees.values(), k=10)\n",
    "                targets_sample = random.choices(list(in_degrees.keys()), weights=in_degrees.values(), k=10)\n",
    "                edge_sample = [\n",
    "                    (source, target) \n",
    "                    for source in sources_sample \n",
    "                    for target in targets_sample \n",
    "                    if source != target and not net_new.has_edge(source, target)]\n",
    "                if edge_sample != []:\n",
    "                    new_edge = random.choice(edge_sample) # Throws an error if no edges are available\n",
    "                    n_edges_added += 1\n",
    "                    net_new.add_edge(*new_edge)\n",
    "                    edges_added_degree_dist += 1\n",
    "        # print(f\"{n_edges_added=:,} edges added\")\n",
    "    print(f\"{edges_added_clustering:,} edges added to increase clustering\")\n",
    "    print(f\"{edges_added_degree_dist:,} edges added based on {target_degree_dist} degree distribution\")\n",
    "    return net_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c43f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify_fancy(\n",
    "    net: nx.DiGraph, n_edges: int, target_degree_dist: str = \"original\", target_clustering: float = None,\n",
    ") -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Densifies a directed network by adding new edges to increase its density, \n",
    "    while optionally targeting a specific degree distribution and clustering coefficient.\n",
    "    Priority is given to targeting the specified clustering coefficient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : nx.DiGraph\n",
    "        The original directed network to densify.\n",
    "    n_edges : int\n",
    "        The number of edges to add.\n",
    "    target_degree_dist : str, optional\n",
    "        The target degree distribution for new edges. \n",
    "        \"original\" preserves the original degree distribution, \n",
    "        \"uniform\" assigns equal probability to all nodes. Default is \"original\".\n",
    "    target_clustering : float, optional\n",
    "        The desired average clustering coefficient. If None, uses the original network's clustering.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nx.DiGraph\n",
    "        A new directed network with increased density and optionally modified clustering/degree distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original network\n",
    "    net_new = copy.deepcopy(net)\n",
    "    \n",
    "    if target_clustering is None:\n",
    "        target_clustering = nx.average_clustering(net)\n",
    "    if target_degree_dist == \"original\":\n",
    "        # Use the original degree distribution\n",
    "        out_degrees = dict(net.out_degree())\n",
    "        in_degrees = dict(net.in_degree())\n",
    "    if target_degree_dist == \"uniform\":\n",
    "        out_degrees = {node: 1 for node in net.nodes()}\n",
    "        in_degrees = {node: 1 for node in net.nodes()}\n",
    "\n",
    "    # Add edges in neighborhoods\n",
    "    n_edges_added = 0\n",
    "    edges_added_clustering = 0\n",
    "    edges_added_degree_dist = 0\n",
    "    new_clustering = nx.average_clustering(net_new)\n",
    "    while n_edges_added < n_edges:\n",
    "        if new_clustering < target_clustering:\n",
    "            # Add new edge to increase clustering\n",
    "            node = random.choice(list(net.nodes()))\n",
    "            neighbors = list(net.predecessors(node)) + list(net.successors(node))\n",
    "            out_degrees_neighbors = {node: out_degrees[node] for node in neighbors}\n",
    "            in_degrees_neighbors = {node: in_degrees[node] for node in neighbors}\n",
    "            out_weights = out_degrees_neighbors.values()\n",
    "            if all(out_weights) == 0:\n",
    "                out_weights = np.ones(len(out_degrees_neighbors.keys()))\n",
    "            in_weights = in_degrees_neighbors.values()\n",
    "        \n",
    "            if all(in_weights) == 0:\n",
    "                in_weights = np.ones(len(in_degrees_neighbors.keys()))\n",
    "            \n",
    "            sources = random.choices(list(out_degrees_neighbors.keys()), weights=out_weights, k=10)\n",
    "            targets = random.choices(list(in_degrees_neighbors.keys()), weights=in_weights, k=10)\n",
    "            possible_edges = [\n",
    "                (source, target) for source in sources for target in targets\n",
    "                if source != target and not net_new.in_edges(source, target)\n",
    "            ]\n",
    "            if possible_edges != []:\n",
    "                new_edge = random.choice(possible_edges)\n",
    "                n_edges_added += 1\n",
    "                net_new.add_edge(*new_edge)\n",
    "                new_clustering = nx.average_clustering(net_new)\n",
    "                edges_added_clustering += 1\n",
    "        else:\n",
    "            # Add new edge based on target degree distribution\n",
    "            sources_sample = random.choices(list(out_degrees.keys()), weights=out_degrees.values(), k=10)\n",
    "            targets_sample = random.choices(list(in_degrees.keys()), weights=in_degrees.values(), k=10)\n",
    "            edge_sample = [\n",
    "                (source, target) \n",
    "                for source in sources_sample \n",
    "                for target in targets_sample \n",
    "                if source != target and not net_new.has_edge(source, target)]\n",
    "            if edge_sample != []:\n",
    "                new_edge = random.choice(edge_sample) # Throws an error if no edges are available\n",
    "                n_edges_added += 1\n",
    "                net_new.add_edge(*new_edge)\n",
    "                new_clustering = nx.average_clustering(net_new)\n",
    "                edges_added_degree_dist += 1\n",
    "        print(f\"{n_edges_added=:,} edges added\")\n",
    "    print(f\"{edges_added_clustering:,} edges added to increase clustering\")\n",
    "    print(f\"{edges_added_degree_dist:,} edges added based on {target_degree_dist} degree distribution\")\n",
    "    return net_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3685c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def densify_fancy_speed_up(\n",
    "    net: nx.DiGraph, n_edges: int, target_degree_dist: str = \"original\", target_average_clustering: float = None,\n",
    ") -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Densifies a directed network by adding new edges to increase its density, \n",
    "    while optionally targeting a specific degree distribution and clustering coefficient.\n",
    "    Priority is given to targeting the specified clustering coefficient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : nx.DiGraph\n",
    "        The original directed network to densify.\n",
    "    n_edges : int\n",
    "        The number of edges to add.\n",
    "    target_degree_dist : str, optional\n",
    "        The target degree distribution for new edges. \n",
    "        \"original\" preserves the original degree distribution, \n",
    "        \"uniform\" assigns equal probability to all nodes. Default is \"original\".\n",
    "    target_clustering : float, optional\n",
    "        The desired average clustering coefficient. If None, uses the original network's clustering.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nx.DiGraph\n",
    "        A new directed network with increased density and optionally modified clustering/degree distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original network\n",
    "    net_new = copy.deepcopy(net)\n",
    "    \n",
    "    if target_average_clustering is None:\n",
    "        target_average_clustering = nx.average_clustering(net)\n",
    "    if target_degree_dist == \"original\":\n",
    "        # Use the original degree distribution\n",
    "        out_degrees = dict(net.out_degree())\n",
    "        in_degrees = dict(net.in_degree())\n",
    "    if target_degree_dist == \"uniform\":\n",
    "        out_degrees = {node: 1 for node in net.nodes()}\n",
    "        in_degrees = {node: 1 for node in net.nodes()}\n",
    "    clustering_dict: dict = nx.clustering(net_new)\n",
    "    \n",
    "    # Add edges in neighborhoods\n",
    "    n_edges_added = 0\n",
    "    edges_added_clustering = 0\n",
    "    edges_added_degree_dist = 0\n",
    "    new_average_clustering = np.average(list(clustering_dict.values()))\n",
    "    while n_edges_added < n_edges:\n",
    "        if new_average_clustering < target_average_clustering:\n",
    "            # Add new edge to increase clustering\n",
    "            node = random.choice(list(net.nodes()))\n",
    "            neighbors = list(net.predecessors(node)) + list(net.successors(node))\n",
    "            out_degrees_neighbors = {node: out_degrees[node] for node in neighbors}\n",
    "            in_degrees_neighbors = {node: in_degrees[node] for node in neighbors}\n",
    "            out_weights = out_degrees_neighbors.values()\n",
    "            if all(out_weights) == 0:\n",
    "                out_weights = np.ones(len(out_degrees_neighbors.keys()))\n",
    "            in_weights = in_degrees_neighbors.values()\n",
    "        \n",
    "            if all(in_weights) == 0:\n",
    "                in_weights = np.ones(len(in_degrees_neighbors.keys()))\n",
    "            \n",
    "            sources = random.choices(list(out_degrees_neighbors.keys()), weights=out_weights, k=10)\n",
    "            targets = random.choices(list(in_degrees_neighbors.keys()), weights=in_weights, k=10)\n",
    "            possible_edges = [\n",
    "                (source, target) for source in sources for target in targets\n",
    "                if source != target and not net_new.in_edges(source, target)\n",
    "            ]\n",
    "            if possible_edges != []:\n",
    "                new_edge = random.choice(possible_edges)\n",
    "                n_edges_added += 1\n",
    "                net_new.add_edge(*new_edge)\n",
    "                neighborhood_0 = list(net_new.predecessors(new_edge[0])) + list(net_new.successors(new_edge[0]))\n",
    "                neighborhood_1 = list(net_new.predecessors(new_edge[1])) + list(net_new.successors(new_edge[1]))\n",
    "                affected_nodes = [new_edge[0], new_edge[1]] + list(set(neighborhood_0).intersection(set(neighborhood_1)))\n",
    "                for node in affected_nodes:\n",
    "                    clustering_dict[node] = nx.clustering(net_new, node)\n",
    "                new_average_clustering = np.average(list(clustering_dict.values()))\n",
    "                edges_added_clustering += 1\n",
    "        else:\n",
    "            # Add new edge based on target degree distribution\n",
    "            sources_sample = random.choices(list(out_degrees.keys()), weights=out_degrees.values(), k=10)\n",
    "            targets_sample = random.choices(list(in_degrees.keys()), weights=in_degrees.values(), k=10)\n",
    "            edge_sample = [\n",
    "                (source, target) \n",
    "                for source in sources_sample \n",
    "                for target in targets_sample \n",
    "                if source != target and not net_new.has_edge(source, target)]\n",
    "            if edge_sample != []:\n",
    "                new_edge = random.choice(edge_sample) # Throws an error if no edges are available\n",
    "                n_edges_added += 1\n",
    "                net_new.add_edge(*new_edge)\n",
    "                neighborhood_0 = list(net_new.predecessors(new_edge[0])) + list(net_new.successors(new_edge[0]))\n",
    "                neighborhood_1 = list(net_new.predecessors(new_edge[1])) + list(net_new.successors(new_edge[1]))\n",
    "                affected_nodes = [new_edge[0], new_edge[1]] + list(set(neighborhood_0).intersection(set(neighborhood_1)))\n",
    "                for node in affected_nodes:\n",
    "                    clustering_dict[node] = nx.clustering(net_new, node)\n",
    "                new_average_clustering = np.average(list(clustering_dict.values()))\n",
    "                edges_added_degree_dist += 1\n",
    "        # print(f\"{n_edges_added=:,} edges added\")\n",
    "    print(f\"{edges_added_clustering:,} edges added to increase clustering\")\n",
    "    print(f\"{edges_added_degree_dist:,} edges added based on {target_degree_dist} degree distribution\")\n",
    "    return net_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78166ffb",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbc91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triangles(G: nx.DiGraph):\n",
    "    \"\"\"Return the list of all triangles in a directed graph G.\"\"\"\n",
    "    triangles = []\n",
    "    for clique in nx.enumerate_all_cliques(G.to_undirected()):\n",
    "        if len(clique) <= 3:\n",
    "            if len(clique) == 3:\n",
    "                triangles.append(clique)\n",
    "        else:\n",
    "            return triangles\n",
    "    return triangles\n",
    "\n",
    "def decluster(net: nx.DiGraph, n_triangles: int) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Decluster the network by rewiring n_triangles random triangles.\n",
    "    \"\"\"\n",
    "    decluster_net = copy.deepcopy(net)\n",
    "    triangles = get_triangles(net)\n",
    "    rewired_triangles = random.sample(triangles, n_triangles)\n",
    "    rewired_edges = [\n",
    "        (source, target) \n",
    "        for (source, target, _) in rewired_triangles\n",
    "    ] # Warning: triangles are based on undirected graph!\n",
    "    \n",
    "    for edge in rewired_edges:\n",
    "        # Remove edge\n",
    "        if decluster_net.has_edge(*edge):\n",
    "            decluster_net.remove_edge(*edge)\n",
    "        elif decluster_net.has_edge(edge[1], edge[0]):\n",
    "            decluster_net.remove_edge(edge[1], edge[0]) \n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Add new edge based on out- and in-degree distribution\n",
    "        out_degrees = dict(net.out_degree())\n",
    "        in_degrees = dict(net.in_degree())\n",
    "        sources_sample = random.choices(list(out_degrees.keys()), weights=out_degrees.values(), k=10)\n",
    "        targets_sample = random.choices(list(in_degrees.keys()), weights=in_degrees.values(), k=10)\n",
    "        edge_sample = [\n",
    "            (source, target) \n",
    "            for source in sources_sample \n",
    "            for target in targets_sample \n",
    "            if source != target and not decluster_net.has_edge(source, target)]\n",
    "        new_edge = random.choice(edge_sample) # Throws an error if no edges are available\n",
    "        decluster_net.add_edge(*new_edge)\n",
    "    return decluster_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b30716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_network(net: nx.DiGraph, n: int) -> nx.DiGraph:\n",
    "    # Create a copy of the original network\n",
    "    cluster_net = copy.deepcopy(net)\n",
    "        \n",
    "    \n",
    "    # Add edges based on the degree distribution\n",
    "    n_edges_to_add = n\n",
    "    print(f\"{n_edges_to_add=:,}\")\n",
    "\n",
    "    # Add edges in neighborhoods\n",
    "    edges_new = []\n",
    "    while len(edges_new) < n_edges_to_add:\n",
    "        node = random.choice(list(net.nodes()))\n",
    "        neighbors = list(net.predecessors(node)) + list(net.successors(node))\n",
    "        out_degrees_neighbors = dict(net.out_degree(neighbors))\n",
    "        in_degrees_neighbors = dict(net.in_degree(neighbors))\n",
    "        out_weights = out_degrees_neighbors.values()\n",
    "        if all(out_weights) == 0:\n",
    "            out_weights = np.ones(len(out_degrees_neighbors.keys()))\n",
    "        in_weights = in_degrees_neighbors.values()\n",
    "    \n",
    "        if all(in_weights) == 0:\n",
    "            in_weights = np.ones(len(in_degrees_neighbors.keys()))\n",
    "        \n",
    "        sources = random.choices(list(out_degrees_neighbors.keys()), weights=out_weights, k=10)\n",
    "        targets = random.choices(list(in_degrees_neighbors.keys()), weights=in_weights, k=10)\n",
    "        possible_edges = [\n",
    "            (source, target) for source in sources for target in targets\n",
    "            if source != target and not (source, target) in edges_new and not net.in_edges(source, target)\n",
    "        ]\n",
    "        if possible_edges != []:\n",
    "            edges_new.append(random.choice(possible_edges))\n",
    "    cluster_net.add_edges_from(edges_new)\n",
    "    \n",
    "    return cluster_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c779626",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
