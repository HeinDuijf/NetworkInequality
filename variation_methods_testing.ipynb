{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main contribution of the paper:\n",
    "\n",
    "- Methodological contribution of empirical robustness for _network_ epistemology. \n",
    "\n",
    "Key challenge:\n",
    "\n",
    "- Test whether inequality (or some other network feature such as density, clustering, or diameter) increases reliability (or some other metric such as speed).\n",
    "- Counterfactual: had the network been more equal, the group would have been more reliable.\n",
    "- This requires us to identify networks that differ in their inequality but are otherwise maximally similar.\n",
    "- We consider three network features: density, clustering, and degree inequality. So networks that have the similar density, clustering and degree inequality are considered maximally similar. (I left out diameter here.)\n",
    "\n",
    "To achieve this, we develop some variation methods. The main goals for these variation methods are:\n",
    "\n",
    "1. Control: Ability to tinker with specific network features (density, inequality, clustering)\n",
    "2. Simplicity\n",
    "3. Computational tractability\n",
    "4. Link to (individualistic) intervention \n",
    "\n",
    "There are two types of variation methods: \n",
    "\n",
    "1. Possibly complex variation methods that can be used to produce networks with specific network properties (density, inequality, clustering), i.e., high control\n",
    "2. Intuitive and simple variation methods that yield a lower degree of control over the specific network properties (density, inequality, clustering)\n",
    "\n",
    "The basic ideas in these variation methods are the following:\n",
    "\n",
    "1. Density\n",
    "    - Increase by adding edges\n",
    "    - Keep constant by rewiring edges\n",
    "    - Decrease by removing edges [not implemented]\n",
    "2. Clustering\n",
    "    - Note: the local clustering coefficient of a given node basically is the number of triangles that pass by that node divided by the number of possible triangles that pass by that node. \n",
    "    - Increase by adding edges that create new triangles\n",
    "    - Keep constant ???\n",
    "    - Decrease by removing edges from existing triangles\n",
    "3. Inequality\n",
    "    - Decrease by adding edges randomly (following the uniform degree distribution)\n",
    "    - Keep constant by adding edges following the original degree distribution (i.e., that of the PUD network)\n",
    "    - Increase by (a) sequentially adding edges preferentially, or  (b) by adding edges following a degree distribution that is more unequal than the original one [neither implemented]\n",
    "\n",
    "## Hop over to Outcomes section to see the results of the different variation methods\n",
    "\n",
    "## Simple variation methods\n",
    "\n",
    "### Equalizers: tinker with inequality\n",
    "\n",
    "1. `randomize_network`: Randomly rewire edges (following the uniform degree distribution) [THIS IS WHAT WE CURRENTLY DO]\n",
    "    - Density $=$\n",
    "    - Clustering $\\downarrow$\n",
    "    - Inequality $\\downarrow$\n",
    "2. `equalize`: Rewire triangles: Take a triangle, take a node in the triangle, remove the edge in the triangle that does not contain the node, then add a random new edge that creates a new triangle that passed by the node. (The goal was to keep clustering somewhat equal, while increasing equality.)\n",
    "    - Density $=$\n",
    "    - Clustering $\\approx \\downarrow$\n",
    "    - Inequality $\\downarrow$\n",
    "\n",
    "### Densify\n",
    "1. [not implemented] Randomly add edges (following the uniform degree distribution)\n",
    "    - Expectations\n",
    "        - Density $\\uparrow$\n",
    "        - Clustering $\\downarrow$\n",
    "        - Inequality $\\downarrow$\n",
    "2. `densify`: Add edges following the original degree distribution\n",
    "    - Density $\\uparrow$\n",
    "    - Clustering $\\downarrow$\n",
    "    - Inequality $=$\n",
    "3. `cluster`: Add edges that create new triangles (taking into account the original degree distribution)\n",
    "    - Density $\\uparrow$\n",
    "    - Clustering $\\Uparrow$\n",
    "    - Inequality $\\approx$ \n",
    "\n",
    "### Clustering\n",
    "1. `decluster`: Remove edges from existing triangles and add new edge following the original degree distribution\n",
    "    - Density $=$\n",
    "    - Clustering $\\Downarrow$\n",
    "    - Inequality $\\approx\\downarrow$\n",
    "\n",
    "\n",
    "## Complex variation methods\n",
    "\n",
    "1. `densify_fancy`: Add edges in such a way to attempt to reach a target clustering coefficient and a target degree distribution (original or uniform). \n",
    "    - Basically, the algorithm checks whether the target clustering coefficient has been reached. If not, it adds an edge that increases clustering. If yes, it adds a new edge following the target degree distribution. \n",
    "    - **Computationally costly**: after an edge is added, the algorithm calculates the clustering coefficient of the new network, which is computationally costly to an unacceptable degree (I think). \n",
    "    - **High degree of control**, especially regarandoming the clustering coefficient: For example, can be used to achieve the following:\n",
    "        1. Only increase density\n",
    "            - Density $\\uparrow$\n",
    "            - Clustering $=$\n",
    "            - Inequality $=$\n",
    "        2. Increase density and decrease inequality\n",
    "            - Density $\\uparrow$\n",
    "            - Clustering $=$\n",
    "            - Inequality $\\downarrow$\n",
    "2. `densify_fancy_speed_up`: This method basically works the same as `densify_fancy`, but has a significant speed up. Instead of calculating the average clustering coefficient whenever an edge is added, we only calculate the new local clusterinf coefficient for nodes that are affected by the newly added edge. \n",
    "    - **Computationally not cheap**: although the algorithm is much quicker than `densify_fancy`, it still takes some time. I expect that the computational costs are acceptable.\n",
    "    - **High control**, especially regarandoming the clustering coefficient.\n",
    "3. `densify_semi_fancy`: Add edges either to increase clustering or following the target degree distribution — with a fixed probability.\n",
    "    - Basically, the algorithm throws a (biased) coin to determine whether it will add an edge that increases clustering or add one following the target degree distribution. \n",
    "    - Computationally fast. Relies on a biased coin toss instead of calculating the clustering coefficient. The drawback is that the clustering coefficient will change to some degree.\n",
    "    - Pretty high degree of control.\n",
    "\n",
    "\n",
    "- Note: we could keep the density fixed by first removing a number of edges and then using `densify_fancy` or `densify_semi_fancy` to add the same number of edges. \n",
    "\n",
    "## Thoughts and observations\n",
    "\n",
    "- I was surprised to learn that both the in-degree and the out-degree distributions of the PUD network are scale free. However, they are not neatly correlated. Nonetheless, my suspicion is that there is a pattern between an agent’s number of publication and both its in-degree and its out-degree. \n",
    "- I was surprised to learn that the clustering coefficient is not based on directed graphs (`nx.average_clustering`). \n",
    "- In a sense, we have currently only implemented the target degree distribution of the degree distribution of the input network (the PUD network) or the uniform distribution. We could consider other degree distributions (i.e., an unequal distribution that flips the original degree distribution)\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imports import *\n",
    "from network_utils import *\n",
    "# import random as random\n",
    "\n",
    "network_pud: nx.DiGraph = dill.load(open('empirical_networks/pud_final.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_net_1 = randomize_network(network_pud, p_rewiring=0.1)\n",
    "random_net_2 = randomize_network(network_pud, p_rewiring=0.2)\n",
    "random_net_3 = randomize_network(network_pud, p_rewiring=0.3)\n",
    "random_net_4 = randomize_network(network_pud, p_rewiring=0.4)\n",
    "random_net_5 = randomize_network(network_pud, p_rewiring=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_random = {\n",
    "    \"network\": [\"original\", \"random_10%\", \"random_20%\", \"random_30%\", \"random_40%\", \"random_50%\"],\n",
    "    \"degree_gini\": [\n",
    "        calculate_degree_gini(network_pud, directed=True),\n",
    "        calculate_degree_gini(random_net_1, directed=True),\n",
    "        calculate_degree_gini(random_net_2, directed=True),\n",
    "        calculate_degree_gini(random_net_3, directed=True),\n",
    "        calculate_degree_gini(random_net_4, directed=True),\n",
    "        calculate_degree_gini(random_net_5, directed=True)\n",
    "    ],\n",
    "    \"clustering_coefficient\": [\n",
    "        nx.average_clustering(network_pud),\n",
    "        nx.average_clustering(random_net_1),\n",
    "        nx.average_clustering(random_net_2),\n",
    "        nx.average_clustering(random_net_3),\n",
    "        nx.average_clustering(random_net_4),\n",
    "        nx.average_clustering(random_net_5)\n",
    "    ],\n",
    "    \"density\": [\n",
    "        nx.density(network_pud),\n",
    "        nx.density(random_net_1),\n",
    "        nx.density(random_net_2),\n",
    "        nx.density(random_net_3),\n",
    "        nx.density(random_net_4),\n",
    "        nx.density(random_net_5)\n",
    "    ]\n",
    "}\n",
    "df_random = pd.DataFrame(data_random)\n",
    "df_random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_net_1 = equalize(network_pud, n=1000)\n",
    "equalized_net_2 = equalize(network_pud, n=2000)\n",
    "equalized_net_3 = equalize(network_pud, n=3000)\n",
    "equalized_net_4 = equalize(network_pud, n=4000)\n",
    "equalized_net_5 = equalize(network_pud, n=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_equalize = {\n",
    "    \"network\": [\"original\", \"equalized_1000\", \"equalized_2000\", \"equalized_3000\", \"equalized_4000\", \"equalized_5000\"],\n",
    "    \"degree_gini\": [\n",
    "        calculate_degree_gini(network_pud, directed=True),\n",
    "        calculate_degree_gini(equalized_net_1, directed=True),\n",
    "        calculate_degree_gini(equalized_net_2, directed=True),\n",
    "        calculate_degree_gini(equalized_net_3, directed=True),\n",
    "        calculate_degree_gini(equalized_net_4, directed=True),\n",
    "        calculate_degree_gini(equalized_net_5, directed=True)\n",
    "    ],\n",
    "    \"clustering_coefficient\": [\n",
    "        nx.average_clustering(network_pud),\n",
    "        nx.average_clustering(equalized_net_1),\n",
    "        nx.average_clustering(equalized_net_2),\n",
    "        nx.average_clustering(equalized_net_3),\n",
    "        nx.average_clustering(equalized_net_4),\n",
    "        nx.average_clustering(equalized_net_5)\n",
    "    ],\n",
    "    \"density\": [\n",
    "        nx.density(network_pud),\n",
    "        nx.density(equalized_net_1),\n",
    "        nx.density(equalized_net_2),\n",
    "        nx.density(equalized_net_3),\n",
    "        nx.density(equalized_net_4),\n",
    "        nx.density(equalized_net_5)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_equalize = pd.DataFrame(data_equalize)\n",
    "df_equalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densify_fancy_speedy_1000 = densify_fancy_speed_up(network_pud, n_edges=1000, target_degree_dist=\"original\")\n",
    "print('Done')\n",
    "densify_fancy_speedy_2000 = densify_fancy_speed_up(network_pud, n_edges=2000, target_degree_dist=\"original\")\n",
    "print('Done')\n",
    "densify_fancy_speedy_3000 = densify_fancy_speed_up(network_pud, n_edges=3000, target_degree_dist=\"original\")\n",
    "print('Done')\n",
    "densify_fancy_speedy_4000 = densify_fancy_speed_up(network_pud, n_edges=4000, target_degree_dist=\"original\")\n",
    "print('Done')\n",
    "densify_fancy_speedy_5000 = densify_fancy_speed_up(network_pud, n_edges=5000, target_degree_dist=\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_densify_fancy = {\n",
    "    \"network\": [\"original\", \"densify_fancy_speedy_1000\", \"densify_fancy_speedy_2000\", \"densify_fancy_speedy_3000\", \"densify_fancy_speedy_4000\", \"densify_fancy_speedy_5000\"], \n",
    "    \"degree_gini\": [\n",
    "        calculate_degree_gini(network_pud, directed=True),\n",
    "        calculate_degree_gini(densify_fancy_speedy_1000, directed=True),\n",
    "        calculate_degree_gini(densify_fancy_speedy_2000, directed=True),\n",
    "        calculate_degree_gini(densify_fancy_speedy_3000, directed=True),\n",
    "        calculate_degree_gini(densify_fancy_speedy_4000, directed=True),\n",
    "        calculate_degree_gini(densify_fancy_speedy_5000, directed=True)\n",
    "    ],\n",
    "    \"clustering_coefficient\": [\n",
    "        nx.average_clustering(network_pud),\n",
    "        nx.average_clustering(densify_fancy_speedy_1000),\n",
    "        nx.average_clustering(densify_fancy_speedy_2000),\n",
    "        nx.average_clustering(densify_fancy_speedy_3000),\n",
    "        nx.average_clustering(densify_fancy_speedy_4000),\n",
    "        nx.average_clustering(densify_fancy_speedy_5000)\n",
    "    ],\n",
    "    \"density\": [\n",
    "        nx.density(network_pud),\n",
    "        nx.density(densify_fancy_speedy_1000),\n",
    "        nx.density(densify_fancy_speedy_2000),\n",
    "        nx.density(densify_fancy_speedy_3000),\n",
    "        nx.density(densify_fancy_speedy_4000),\n",
    "        nx.density(densify_fancy_speedy_5000)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_densify_fancy_speedy = pd.DataFrame(data_densify_fancy)\n",
    "df_densify_fancy_speedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decluster_net_1 = decluster(network_pud, n_triangles=1000)\n",
    "print(\"Done\")\n",
    "decluster_net_2 = decluster(network_pud, n_triangles=2000)\n",
    "print(\"Done\")\n",
    "decluster_net_3 = decluster(network_pud, n_triangles=3000)    \n",
    "print(\"Done\")\n",
    "decluster_net_4 = decluster(network_pud, n_triangles=4000)\n",
    "print(\"Done\")\n",
    "decluster_net_5 = decluster(network_pud, n_triangles=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_decluster = {\n",
    "    \"network\": [\"original\", \"declustered_1000\", \"declustered_2000\", \"declustered_3000\", \"declustered_4000\", \"declustered_5000\"],\n",
    "    \"degree_gini\": [\n",
    "        calculate_degree_gini(network_pud, directed=True),\n",
    "        calculate_degree_gini(decluster_net_1, directed=True),\n",
    "        calculate_degree_gini(decluster_net_2, directed=True),\n",
    "        calculate_degree_gini(decluster_net_3, directed=True),\n",
    "        calculate_degree_gini(decluster_net_4, directed=True),\n",
    "        calculate_degree_gini(decluster_net_5, directed=True)\n",
    "    ],\n",
    "    \"clustering_coefficient\": [\n",
    "        nx.average_clustering(network_pud),\n",
    "        nx.average_clustering(decluster_net_1),\n",
    "        nx.average_clustering(decluster_net_2),\n",
    "        nx.average_clustering(decluster_net_3),\n",
    "        nx.average_clustering(decluster_net_4),\n",
    "        nx.average_clustering(decluster_net_5)\n",
    "    ],\n",
    "    \"density\": [\n",
    "        nx.density(network_pud),\n",
    "        nx.density(decluster_net_1),\n",
    "        nx.density(decluster_net_2),\n",
    "        nx.density(decluster_net_3),\n",
    "        nx.density(decluster_net_4),\n",
    "        nx.density(decluster_net_5)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_decluster = pd.DataFrame(data_decluster)\n",
    "df_decluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_network_1 = cluster_network(network_pud, n=1000)\n",
    "print(\"Done\")\n",
    "cluster_network_2 = cluster_network(network_pud, n=2000)\n",
    "print(\"Done\")\n",
    "cluster_network_3 = cluster_network(network_pud, n=3000)\n",
    "print(\"Done\")\n",
    "cluster_network_4 = cluster_network(network_pud, n=4000)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cluster = {\n",
    "    \"network\": [\"original\", \"clustered_1000\", \"clustered_2000\", \"clustered_3000\", \"clustered_4000\"],\n",
    "    \"degree_gini\": [\n",
    "        calculate_degree_gini(network_pud, directed=True),\n",
    "        calculate_degree_gini(cluster_network_1, directed=True),\n",
    "        calculate_degree_gini(cluster_network_2, directed=True),\n",
    "        calculate_degree_gini(cluster_network_3, directed=True),\n",
    "        calculate_degree_gini(cluster_network_4, directed=True)\n",
    "    ],\n",
    "    \"clustering_coefficient\": [\n",
    "        nx.average_clustering(network_pud),\n",
    "        nx.average_clustering(cluster_network_1),\n",
    "        nx.average_clustering(cluster_network_2),\n",
    "        nx.average_clustering(cluster_network_3),\n",
    "        nx.average_clustering(cluster_network_4)\n",
    "    ],\n",
    "    \"density\": [\n",
    "        nx.density(network_pud),\n",
    "        nx.density(cluster_network_1),\n",
    "        nx.density(cluster_network_2),\n",
    "        nx.density(cluster_network_3),\n",
    "        nx.density(cluster_network_4)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_cluster = pd.DataFrame(data_cluster)\n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_densify_fancy_speedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
